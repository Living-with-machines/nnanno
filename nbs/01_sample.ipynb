{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample\n",
    "\n",
    "> Create samples from Newspaper navigator dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\ # TODO Intro to module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from nnanno.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# TODO tidy imports\n",
    "\n",
    "# sys \n",
    "import io\n",
    "import shutil\n",
    "import pkg_resources\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# other\n",
    "from tqdm.auto import trange, tqdm\n",
    "import requests\n",
    "import ijson\n",
    "import functools\n",
    "import math\n",
    "from cytoolz import dicttoolz, itertoolz\n",
    "import random\n",
    "import json\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "import itertools\n",
    "from pandas import json_normalize\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "# typing\n",
    "from typing import (\n",
    "    Any,\n",
    "    Optional,\n",
    "    Union,\n",
    "    Dict,\n",
    "    List,\n",
    "    Tuple,\n",
    "    Set,\n",
    "    Iterable,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_json_url(year: Union[str,int], kind:str='photos') -> str:\n",
    "    '''Returns url for the json data from news-navigator for given `year` and `kind`'''\n",
    "    return f'https://news-navigator.labs.loc.gov/prepackaged/{year}_{kind}.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_json_url(1860) == 'https://news-navigator.labs.loc.gov/prepackaged/1860_photos.json' \n",
    "assert get_json_url(1950) == 'https://news-navigator.labs.loc.gov/prepackaged/1950_photos.json' \n",
    "assert get_json_url(1950,'ads') == 'https://news-navigator.labs.loc.gov/prepackaged/1950_ads.json' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_json(url) -> Dict[str, Any]:\n",
    "    \"\"\"Returns loaded json from url\n",
    "\n",
    "    Parameters:\n",
    "    url (str): URL for news-navigator json file\n",
    "\n",
    "    Returns:\n",
    "    Dict: dictionary with data from input json url\n",
    "    \"\"\"\n",
    "    with requests.get(url, timeout=2) as r:\n",
    "        r.raise_for_status()\n",
    "        return json.loads(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_json = load_json('https://news-navigator.labs.loc.gov/prepackaged/1950_photos.json')\n",
    "assert type(test_json[0]) == dict\n",
    "assert type(test_json) == list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works well for a smallish file but if we try this with the [1905_ads.json](https://news-navigator.labs.loc.gov/prepackaged/1910_ads.json) file which is ~3.3GB we will likely run out of memory. For example running \n",
    "\n",
    "```python\n",
    "with requests.get('https://news-navigator.labs.loc.gov/prepackaged/1910_ads.json') as r:\n",
    "    data = json.loads(r.content)\n",
    "len(data)\n",
    "```\n",
    "\n",
    "on a Google Colab instance with 25GB of RAM causes a crash. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@functools.lru_cache(256)\n",
    "def count_json_iter(url: str, session=None) -> int:\n",
    "    \"\"\"\n",
    "    Returns count of objects in url json file using an iterator to avoid loading json          into memory\n",
    "\n",
    "    Parameters:\n",
    "    url (str): URL for news-navigator json file\n",
    "\n",
    "    Returns:\n",
    "    int: count of json objects in url\n",
    "    \"\"\"\n",
    "    if not session:\n",
    "        session = create_cached_session()\n",
    "    with session.get(url, timeout=60) as r:\n",
    "        r.raise_for_status()\n",
    "        if r:\n",
    "            objects = ijson.items(r.content, \"item\")\n",
    "            count = itertoolz.count(iter(objects))\n",
    "        else:\n",
    "            count = np.nan\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_json_iter('https://news-navigator.labs.loc.gov/prepackaged/1850_photos.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://news-navigator.labs.loc.gov/prepackaged/1850_photos.json'\n",
    "assert type(count_json_iter(url)) == int\n",
    "assert len(json.loads(requests.get(url).content)) == count_json_iter(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@functools.lru_cache(256)\n",
    "def get_year_size(year,kind):\n",
    "    session = None\n",
    "    dset_size = {}       \n",
    "    url = get_json_url(year,kind)\n",
    "    if kind == ('ads' or 'headlines') and int(year) >=1870:\n",
    "        session = create_session()\n",
    "    dset_size[str(year)] = count_json_iter(url, session)\n",
    "    return dset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1850': 22}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_year_size(1850, 'photos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@functools.lru_cache(512)\n",
    "def get_year_sizes(kind,start=1850, end=1950, step=5):\n",
    "    \"\"\"\n",
    "    Returns the sizes for json data files for `kind` between year `start` and `end`    \n",
    "    with step size 'step'\n",
    "\n",
    "    Parameters:\n",
    "    kind (str): kind of image from news-navigator options:\n",
    "    photos, illustrations, maps, comics, cartoons, headlines, ads\n",
    "\n",
    "    Returns:\n",
    "    Pandas.DataFrame: with data from input json url\n",
    "    \"\"\"\n",
    "  #  dset_size = {}\n",
    "    futures = []\n",
    "    years = range(start,end+1,step)\n",
    "    max_workers = get_max_workers(years)\n",
    "    with tqdm(total=len(years)) as progress:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            for year in years:\n",
    "                future = executor.submit(get_year_size, year, kind)\n",
    "                future.add_done_callback(lambda p: progress.update())\n",
    "                futures.append(future)\n",
    "        results = [future.result() for future in futures]\n",
    "        dset_size = {k: v for d in results for k, v in d.items()}\n",
    "    year_df = pd.DataFrame.from_dict(dset_size,orient='index',columns=[f'{kind}_count'])\n",
    "    return year_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the year sizes for a given kind taking a step size `step`. For example to get the number of photos in the news-navigator dataset between 1850 and 1860 for every year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 134.02it/s]CPU times: user 52.4 ms, sys: 53.6 ms, total: 106 ms\n",
      "Wall time: 121 ms\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>photos_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1850</th>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>1851</th>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>1852</th>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>1853</th>\n      <td>45</td>\n    </tr>\n    <tr>\n      <th>1854</th>\n      <td>221</td>\n    </tr>\n    <tr>\n      <th>1855</th>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>1856</th>\n      <td>49</td>\n    </tr>\n    <tr>\n      <th>1857</th>\n      <td>74</td>\n    </tr>\n    <tr>\n      <th>1858</th>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>1859</th>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>1860</th>\n      <td>88</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": [
       "      photos_count\n",
       "1850            22\n",
       "1851            20\n",
       "1852            22\n",
       "1853            45\n",
       "1854           221\n",
       "1855            17\n",
       "1856            49\n",
       "1857            74\n",
       "1858            39\n",
       "1859            28\n",
       "1860            88"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_year_sizes('photos',1850, 1860, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 5343.06it/s]\n"
     ]
    }
   ],
   "source": [
    "assert len(get_year_sizes('photos',1850, 1860, step=1)) == 11\n",
    "assert len(get_year_sizes('photos',1850,1860, step=2)) == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_all_year_sizes(start=1850, end=1950,step=1, save:bool=True):\n",
    "    \"\"\"\n",
    "    Returns a dataframe with number of counts from year `start` to `end`\n",
    "    \"\"\"\n",
    "    kinds = ['ads', 'photos', 'maps', 'illustrations', 'comics', 'cartoons','headlines']\n",
    "    dfs = []\n",
    "    for kind in tqdm(kinds):\n",
    "        df = get_year_sizes(kind, start=start, end=end,step=step)\n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs, axis=1)\n",
    "    df['total'] = df.sum(axis=1)\n",
    "    if save:\n",
    "        df.to_csv('all_year_sizes.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming sampling\n",
    "\n",
    "Since we want a subset of the Newspaper Navigator datasets which we can either work with for [annotation](!TODO add link) or for inference we want to create samples. Sampling in python can be complicated depending on the type of population you are working with and the properties your sample needs to have but usually we can do something fairly simple like. For example, if we want to sample from a selection of books we could do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Frankenstein']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "books = ['War and Peace', 'Frankenstein', 'If They Come in the Morning']\n",
    "random.sample(books, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we run into a same problem as when trying to get the length of a json dataset which wouldn't fit into memory above. For example if we want to sample $k$ examples from one of our json files which we can't load into memory. To get around this we can use [Reservoir_sampling](https://en.wikipedia.org/wiki/Reservoir_sampling):\n",
    "\n",
    "> Reservoir sampling is a family of randomized algorithms for choosing a simple random sample without replacement of k items from a population of unknown size n in a single pass over the items. The size of the population n is not known to the algorithm and is typically too large to fit all n items into main memory. The population is revealed to the algorithm over time, and the algorithm cannot look back at previous items. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sample_stream(stream, k:int):\n",
    "    \"\"\"\n",
    "    Return a random sample of k elements drawn without replacement from stream.\n",
    "    Designed to be used when the elements of stream cannot easily fit into memory.\n",
    "    \"\"\"\n",
    "    r = np.array(list(itertools.islice(stream, k)))\n",
    "    for t, x in enumerate(stream, k + 1):\n",
    "        i = np.random.randint(1, t + 1)\n",
    "\n",
    "        if i <= k:\n",
    "            r[i - 1] = x\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we sample whilst only loading a small numer of items into memory at one time. This does come at some cost, mainly speed. There are faster ways of sampling from a stream but this isn't the main bottle neck for sampling in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([42723,  1354, 86457, 75827, 75402])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_stream(range(1,100000), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Raya Dunayevsk', 'Rosa Luxenburg'], dtype='<U14')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['Karl Marx', 'Rosa Luxenburg', 'Raya Dunayevskaya', 'CLR James']\n",
    "sample_stream(iter(names), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert len(sample_stream(range(1,100),5)) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@functools.lru_cache(128)\n",
    "def calc_frac_size(url,frac):\n",
    "    \"returns fraction size from a json stream\"\n",
    "    return round(count_json_iter(url)*frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "url = get_json_url(1850)\n",
    "assert calc_frac_size(url, 0.5)== 11 #22*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def calc_year_from_total(total,start,end,step):\n",
    "    \"Calculate size of a year sample based on a total sample size\"\n",
    "    return max(1,round(total/(((end-start)+1)/step)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_year_from_total(10,1850, 18950,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "assert calc_year_from_total(10, 1850,1950,1) >=1 # test that a value is always returned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def reduce_df_memory(df):\n",
    "    return df.astype(\n",
    "            {\"score\": \"float64\",\n",
    "                \"page_seq_num\": \"int32\",\n",
    "                \"batch\": \"category\",\n",
    "                \"box\":\"object\",\n",
    "                \"lccn\": \"category\",\n",
    "                \"page_url\": \"category\",\n",
    "                \"name\": \"category\",\n",
    "                \"publisher\": \"category\",\n",
    "                \"place_of_publication\": \"category\",\n",
    "                \"edition_seq_num\": \"category\"}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def sample_year(kind:str,sample_size:Union[int,float], year:int) ->np.array:\n",
    "    url = get_json_url(year, kind)\n",
    "    if type(sample_size) is float:\n",
    "        sample_size = calc_frac_size(url, sample_size)\n",
    "    if kind == ('ads' or 'headlines') and int(year) >=1870:\n",
    "        session = create_session()\n",
    "    else:\n",
    "        session = create_cached_session()\n",
    "    with session.get(get_json_url(year, kind)) as r:\n",
    "        if r:\n",
    "            try:\n",
    "                data = ijson.items(r.content, \"item\")\n",
    "                sample_data = sample_stream(iter(data), sample_size)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                sample_data = np.nan\n",
    "    return sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([{'filepath': 'ohi_ingstad_ver01/data/sn85026051/00296027029/1850121401/0120/000_0_93.jpg', 'pub_date': '1850-12-14', 'page_seq_num': 120, 'edition_seq_num': 1, 'batch': 'ohi_ingstad_ver01', 'lccn': 'sn85026051', 'box': [Decimal('0.3187102596316717'), Decimal('0.6281719624215338'), Decimal('0.4516467879303789'), Decimal('0.7379311907239355')], 'score': Decimal('0.9390530586242676'), 'ocr': ['COME', 'IN,', 'WE', 'CALL', 'YOU!'], 'place_of_publication': 'Fremont, Sandusky County, Ohio', 'geographic_coverage': ['Ohio--Sandusky--Fremont'], 'name': 'Fremont weekly freeman. [volume]', 'publisher': 'J.S. Fouke', 'url': 'https://news-navigator.labs.loc.gov/data/ohi_ingstad_ver01/data/sn85026051/00296027029/1850121401/0120/000_0_93.jpg', 'page_url': 'https://chroniclingamerica.loc.gov/data/batches/ohi_ingstad_ver01/data/sn85026051/00296027029/1850121401/0120.jp2'}],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_year('photos', 1, 1850)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class nnSampler:\n",
    "    def __init__(self):\n",
    "        self.population = pd.read_csv(pkg_resources.resource_stream('nnanno', 'data/all_year_counts.csv'), \n",
    "                                      index_col=0)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (f'{self.__class__.__name__}')\n",
    "        \n",
    "\n",
    "    def create_sample(\n",
    "        self,\n",
    "        sample_size: Union[int, float],\n",
    "        kind: str = \"photos\",\n",
    "        start_year: int = 1850,\n",
    "        end_year: int = 1950,\n",
    "        step: int = 5,\n",
    "        year_sample=True,\n",
    "        save: bool = False,\n",
    "        reduce_memory=True,\n",
    "    ):\n",
    "        if not year_sample:\n",
    "            if not type(sample_size) == int:\n",
    "                raise ValueError(\n",
    "                    f\"type{sample_size} is not an int. Fractions are only supported for sampling by year\"\n",
    "                )\n",
    "            sample_size = calc_year_from_total(sample_size, start_year, end_year, step)\n",
    "        futures = []\n",
    "        years = range(start_year, end_year + 1, step)\n",
    "        _year_sample = partial(sample_year, kind, sample_size)\n",
    "        with tqdm(total=len(years)) as progress:\n",
    "            with concurrent.futures.ThreadPoolExecutor(2) as executor:\n",
    "                for year in years:\n",
    "                    future = executor.submit(_year_sample, year)\n",
    "                    future.add_done_callback(lambda p: progress.update())\n",
    "                    futures.append(future)\n",
    "        results = [future.result() for future in futures]\n",
    "        df = pd.DataFrame.from_dict(list(itertoolz.concat(results)))\n",
    "\n",
    "        if reduce_memory:\n",
    "            df = reduce_df_memory(df)\n",
    "        if save:\n",
    "            df.to_json(f\"{kind}_{start_year}_{end_year}_sample.json\")\n",
    "        self.sample = df\n",
    "        return df\n",
    "\n",
    "    def download_sample(\n",
    "        self,\n",
    "        out_dir,\n",
    "        csv_name=None,\n",
    "        df=None,\n",
    "        original: bool = True,\n",
    "        pct: int = None,\n",
    "        size: tuple = None,\n",
    "        preserve_asp_ratio: bool = True,\n",
    "    ):\n",
    "        if df is not None:\n",
    "            self.download_df = df.copy(deep=True)\n",
    "        else:\n",
    "            try:\n",
    "                self.download_df = self.sample.copy(deep=True)\n",
    "            except AttributeError as E:\n",
    "                print(\n",
    "                    \"You need to create a sample before downloading, or pass in a previously created \"\n",
    "                )\n",
    "        self.download_df[\"iiif_url\"] = self.download_df.apply(\n",
    "            lambda x: iif_df_apply(\n",
    "                x,\n",
    "                original=original,\n",
    "                pct=pct,\n",
    "                size=size,\n",
    "                preserve_asp_ratio=preserve_asp_ratio,\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        self.download_df[\"download_image_path\"] = self.download_df['filepath'].str.replace('/','_')\n",
    "        \n",
    "        if not Path(out_dir).exists():\n",
    "            Path(out_dir).mkdir(parents=True)\n",
    "        _download_image = lambda x: download_image(\n",
    "            x.iiif_url, x.download_image_path, out_dir)\n",
    "        with tqdm(total=len(self.download_df)) as progress:\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                futures = []\n",
    "                for tuple_row in self.download_df.itertuples():\n",
    "                    future = executor.submit(_download_image, tuple_row)\n",
    "                    future.add_done_callback(lambda p: progress.update())\n",
    "                    futures.append(future)\n",
    "                del futures\n",
    "        if csv_name is None:\n",
    "            today = datetime.today()\n",
    "            time_stamp = today.strftime(\"%Y_%d_%m_%H_%M\")\n",
    "            csv_name = f\"{time_stamp}_{len(self.download_df)}_sample\"\n",
    "        #self.download_df.to_csv(f'{out_dir}/{csv_name}.csv')\n",
    "        self.download_df.to_json(f'{out_dir}/{csv_name}.json') # TODO make sure to use json for saving outputs of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = nnSampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nnSampler"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ads_count               63338192\n",
       "photos_count             1543525\n",
       "maps_count                200188\n",
       "illustrations_count       798475\n",
       "comics_count              526319\n",
       "cartoons_count            206054\n",
       "headlines_count         34155014\n",
       "total                  100767767\n",
       "dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.population.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filepath</th>\n      <th>pub_date</th>\n      <th>page_seq_num</th>\n      <th>edition_seq_num</th>\n      <th>batch</th>\n      <th>lccn</th>\n      <th>box</th>\n      <th>score</th>\n      <th>ocr</th>\n      <th>place_of_publication</th>\n      <th>geographic_coverage</th>\n      <th>name</th>\n      <th>publisher</th>\n      <th>url</th>\n      <th>page_url</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pst_intramural_ver01/data/sn83032276/002960285...</td>\n      <td>1850-04-26</td>\n      <td>215</td>\n      <td>1</td>\n      <td>pst_intramural_ver01</td>\n      <td>sn83032276</td>\n      <td>[0.7893347074607134, 0.5260661138779277, 0.944...</td>\n      <td>0.923698</td>\n      <td>[j., w,, PISIITeKJ, Attorney, at, L, n, w,, LE...</td>\n      <td>Lewistown, Pa.</td>\n      <td>[Pennsylvania--Mifflin--Lewistown]</td>\n      <td>Lewistown gazette. [volume]</td>\n      <td>William Ross</td>\n      <td>https://news-navigator.labs.loc.gov/data/pst_i...</td>\n      <td>https://chroniclingamerica.loc.gov/data/batche...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>msar_ikat_ver01/data/sn87090283/00199917365/18...</td>\n      <td>1850-01-18</td>\n      <td>194</td>\n      <td>1</td>\n      <td>msar_ikat_ver01</td>\n      <td>sn87090283</td>\n      <td>[0.7196589680100182, 0.9352155295751428, 0.851...</td>\n      <td>0.943004</td>\n      <td>[Salt., mr, SACKS, SALT—Just, received, per, f...</td>\n      <td>Port Gibson, Claiborne Co., Miss.</td>\n      <td>[Mississippi--Claiborne--Port Gibson]</td>\n      <td>The Port Gibson herald, and correspondent.</td>\n      <td>Wm. F. Eisely</td>\n      <td>https://news-navigator.labs.loc.gov/data/msar_...</td>\n      <td>https://chroniclingamerica.loc.gov/data/batche...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>dlc_dragon_ver01/data/sn82014434/00415661381/1...</td>\n      <td>1850-09-24</td>\n      <td>315</td>\n      <td>1</td>\n      <td>dlc_dragon_ver01</td>\n      <td>sn82014434</td>\n      <td>[0.055765863577795595, 0.9405716126223644, 0.1...</td>\n      <td>0.901824</td>\n      <td>[OK, HURCBVI, WRITIBOB., *'/\\PITM, EATER\", and...</td>\n      <td>Washington [D.C.]</td>\n      <td>[District of Columbia--Washington]</td>\n      <td>The republic. [volume]</td>\n      <td>Gideon &amp; Co.</td>\n      <td>https://news-navigator.labs.loc.gov/data/dlc_d...</td>\n      <td>https://chroniclingamerica.loc.gov/data/batche...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ohi_carrie_ver01/data/sn88078466/00279556161/1...</td>\n      <td>1850-07-10</td>\n      <td>621</td>\n      <td>1</td>\n      <td>ohi_carrie_ver01</td>\n      <td>sn88078466</td>\n      <td>[0.6551259316584248, 0.8280968066708592, 0.799...</td>\n      <td>0.907233</td>\n      <td>[r5, i, s, e, n, S, o, von, joiix, TEai.\\'is,,...</td>\n      <td>Canton, Stark County, Ohio</td>\n      <td>[Ohio--Stark--Canton]</td>\n      <td>Ohio Staats-bote.</td>\n      <td>D.F. Rothnagel</td>\n      <td>https://news-navigator.labs.loc.gov/data/ohi_c...</td>\n      <td>https://chroniclingamerica.loc.gov/data/batche...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ncu_jefferson_ver02/data/sn84020750/0041566727...</td>\n      <td>1850-05-11</td>\n      <td>507</td>\n      <td>1</td>\n      <td>ncu_jefferson_ver02</td>\n      <td>sn84020750</td>\n      <td>[0.6733089634841893, 0.2027780181986831, 0.826...</td>\n      <td>0.937159</td>\n      <td>[TIN, WARE,, At, Wholesale, and, Retail., F., ...</td>\n      <td>Fayetteville [N.C.]</td>\n      <td>[North Carolina--Cumberland--Fayetteville]</td>\n      <td>The North-Carolinian. [volume]</td>\n      <td>H.L. Holmes</td>\n      <td>https://news-navigator.labs.loc.gov/data/ncu_j...</td>\n      <td>https://chroniclingamerica.loc.gov/data/batche...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": [
       "                                            filepath    pub_date  \\\n",
       "0  pst_intramural_ver01/data/sn83032276/002960285...  1850-04-26   \n",
       "1  msar_ikat_ver01/data/sn87090283/00199917365/18...  1850-01-18   \n",
       "2  dlc_dragon_ver01/data/sn82014434/00415661381/1...  1850-09-24   \n",
       "3  ohi_carrie_ver01/data/sn88078466/00279556161/1...  1850-07-10   \n",
       "4  ncu_jefferson_ver02/data/sn84020750/0041566727...  1850-05-11   \n",
       "\n",
       "   page_seq_num edition_seq_num                 batch        lccn  \\\n",
       "0           215               1  pst_intramural_ver01  sn83032276   \n",
       "1           194               1       msar_ikat_ver01  sn87090283   \n",
       "2           315               1      dlc_dragon_ver01  sn82014434   \n",
       "3           621               1      ohi_carrie_ver01  sn88078466   \n",
       "4           507               1   ncu_jefferson_ver02  sn84020750   \n",
       "\n",
       "                                                 box     score  \\\n",
       "0  [0.7893347074607134, 0.5260661138779277, 0.944...  0.923698   \n",
       "1  [0.7196589680100182, 0.9352155295751428, 0.851...  0.943004   \n",
       "2  [0.055765863577795595, 0.9405716126223644, 0.1...  0.901824   \n",
       "3  [0.6551259316584248, 0.8280968066708592, 0.799...  0.907233   \n",
       "4  [0.6733089634841893, 0.2027780181986831, 0.826...  0.937159   \n",
       "\n",
       "                                                 ocr  \\\n",
       "0  [j., w,, PISIITeKJ, Attorney, at, L, n, w,, LE...   \n",
       "1  [Salt., mr, SACKS, SALT—Just, received, per, f...   \n",
       "2  [OK, HURCBVI, WRITIBOB., *'/\\PITM, EATER\", and...   \n",
       "3  [r5, i, s, e, n, S, o, von, joiix, TEai.\\'is,,...   \n",
       "4  [TIN, WARE,, At, Wholesale, and, Retail., F., ...   \n",
       "\n",
       "                place_of_publication  \\\n",
       "0                     Lewistown, Pa.   \n",
       "1  Port Gibson, Claiborne Co., Miss.   \n",
       "2                  Washington [D.C.]   \n",
       "3         Canton, Stark County, Ohio   \n",
       "4                Fayetteville [N.C.]   \n",
       "\n",
       "                          geographic_coverage  \\\n",
       "0          [Pennsylvania--Mifflin--Lewistown]   \n",
       "1       [Mississippi--Claiborne--Port Gibson]   \n",
       "2          [District of Columbia--Washington]   \n",
       "3                       [Ohio--Stark--Canton]   \n",
       "4  [North Carolina--Cumberland--Fayetteville]   \n",
       "\n",
       "                                         name       publisher  \\\n",
       "0                 Lewistown gazette. [volume]    William Ross   \n",
       "1  The Port Gibson herald, and correspondent.   Wm. F. Eisely   \n",
       "2                      The republic. [volume]    Gideon & Co.   \n",
       "3                           Ohio Staats-bote.  D.F. Rothnagel   \n",
       "4              The North-Carolinian. [volume]     H.L. Holmes   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://news-navigator.labs.loc.gov/data/pst_i...   \n",
       "1  https://news-navigator.labs.loc.gov/data/msar_...   \n",
       "2  https://news-navigator.labs.loc.gov/data/dlc_d...   \n",
       "3  https://news-navigator.labs.loc.gov/data/ohi_c...   \n",
       "4  https://news-navigator.labs.loc.gov/data/ncu_j...   \n",
       "\n",
       "                                            page_url  \n",
       "0  https://chroniclingamerica.loc.gov/data/batche...  \n",
       "1  https://chroniclingamerica.loc.gov/data/batche...  \n",
       "2  https://chroniclingamerica.loc.gov/data/batche...  \n",
       "3  https://chroniclingamerica.loc.gov/data/batche...  \n",
       "4  https://chroniclingamerica.loc.gov/data/batche...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sampler.create_sample(sample_size=10, kind='ads', start_year=1850,end_year=1860,reduce_memory=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading a sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-0b7e9be27871>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ads'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_year\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1900\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend_year\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1920\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreduce_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_iif'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-108-e36c6ad5445c>\u001b[0m in \u001b[0;36mcreate_sample\u001b[0;34m(self, sample_size, kind, start_year, end_year, step, year_sample, save, reduce_memory)\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_year_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_done_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                     \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertoolz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/nnAnnotator/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/nnAnnotator/lib/python3.8/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m     \u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/nnAnnotator/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/nnAnnotator/lib/python3.8/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sampler.create_sample(sample_size=10, kind='ads', start_year=1900,end_year=1920,reduce_memory=False)\n",
    "sampler.download_sample('test_iif',pct=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  7.42it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 15.86it/s]\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "sampler = nnSampler()\n",
    "sampler.create_sample(5, step=50)\n",
    "sampler.download_sample('test_iif',pct=5)\n",
    "files = [f for f in Path('test_iif').iterdir()]\n",
    "assert type(Image.open(files[0])) == PIL.JpegImagePlugin.JpegImageFile\n",
    "shutil.rmtree('test_iif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\nConverted 01_sample.ipynb.\nConverted 02_annotate.ipynb.\nConverted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnAnnotator",
   "language": "python",
   "name": "nnannotator"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample\n",
    "\n",
    "> Create samples from Newspaper navigator dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\ # TODO Intro to module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from nnanno.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#TODO tidy imports\n",
    "# sys\n",
    "import io\n",
    "import shutil\n",
    "import pkg_resources\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# other\n",
    "from tqdm.auto import trange, tqdm\n",
    "import requests\n",
    "import ijson\n",
    "import functools\n",
    "import math\n",
    "from cytoolz import dicttoolz, itertoolz\n",
    "import random\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "import itertools\n",
    "from pandas import json_normalize\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from fastcore.foundation import patch_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import PIL\n",
    "from typing import (\n",
    "    Any,\n",
    "    Optional,\n",
    "    Union,\n",
    "    Dict,\n",
    "    List,\n",
    "    Tuple,\n",
    "    Set,\n",
    "    Iterable,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newspaper Navigator JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_json_url(year: Union[str,int], kind:str='photos') -> str:\n",
    "    '''Returns url for the json data from news-navigator for given `year` and `kind`'''\n",
    "    return f'https://news-navigator.labs.loc.gov/prepackaged/{year}_{kind}.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_json_url(1860) == 'https://news-navigator.labs.loc.gov/prepackaged/1860_photos.json' \n",
    "assert get_json_url(1950) == 'https://news-navigator.labs.loc.gov/prepackaged/1950_photos.json' \n",
    "assert get_json_url(1950,'ads') == 'https://news-navigator.labs.loc.gov/prepackaged/1950_ads.json' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_json(url) -> Dict[str, Any]:\n",
    "    \"\"\"Returns json loaded from `url`\"\"\"\n",
    "    with requests.get(url, timeout=2) as r:\n",
    "        r.raise_for_status()\n",
    "        return json.loads(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also test that this returns what we think inside the notebook. These tests are often hidden in the documentation but inside the notebook there will often be a cell below a function which includes some tests for the function which has just been defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_json = load_json('https://news-navigator.labs.loc.gov/prepackaged/1950_photos.json')\n",
    "assert type(test_json[0]) == dict\n",
    "assert type(test_json) == list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with big JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works well for a smallish file but if we try this with the [1905_ads.json](https://news-navigator.labs.loc.gov/prepackaged/1910_ads.json) file which is ~3.3GB we will likely run out of memory. For example running \n",
    "\n",
    "```python\n",
    "with requests.get('https://news-navigator.labs.loc.gov/prepackaged/1910_ads.json') as r:\n",
    "    data = json.loads(r.content)\n",
    "len(data)\n",
    "```\n",
    "\n",
    "on a Google Colab instance with 25GB of RAM causes a crash. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming JSON\n",
    "One way to get around this would be to throw more RAM at the problem. However since we only want to sample the JSON and don't need to work with the whole dataset this seems a bit wasteful. Instead we'll `ijson` a Python library for streaming JSON.\n",
    "\n",
    "We can see how this works for a url from newspaper navigator. If we create a request via Requests using `stream=True` to return a streaming version of the response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(get_json_url(1850, 'ads'), stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass this response to `ijson`. In this case we just parse an item at a time. If the JSON is really big this might already be too much. `ijson` allows for much more granular parsing of JSON but for what we need, parsing by item is fine. We can see what the return of this looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_yajl2.items at 0x104185d50>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects = ijson.items(r.raw, \"item\")\n",
    "objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get back something from `_yajl2` this is the underlying parser ijson is using. See `ijson` docs for more on available parsers. \n",
    "\n",
    "We can call next on this object to start iterating over it, one item at a time. If we look at the keys of the first response you'll see that this is one entry from the original JSON data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['filepath', 'pub_date', 'page_seq_num', 'edition_seq_num', 'batch', 'lccn', 'box', 'score', 'ocr', 'place_of_publication', 'geographic_coverage', 'name', 'publisher', 'url', 'page_url'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first = next(objects)\n",
    "first.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the size of the data \n",
    "\n",
    "\n",
    "- what \n",
    "- how\n",
    "- lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@functools.lru_cache(256)\n",
    "def count_json_iter(url: str, session=None) -> int:\n",
    "    \"\"\"Returns count of objects in url json file using an iterator to avoid loading json into memory\"\"\"\n",
    "    if not session:\n",
    "        session = create_cached_session()\n",
    "    with session.get(url, timeout=60) as r:\n",
    "        r.raise_for_status()\n",
    "        if r:\n",
    "            objects = ijson.items(r.content, \"item\")\n",
    "            count = itertoolz.count(iter(objects))\n",
    "        else:\n",
    "            count = np.nan\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`count_json_iter` counts the length of a json file loaded via `url`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_json_iter('https://news-navigator.labs.loc.gov/prepackaged/1850_photos.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://news-navigator.labs.loc.gov/prepackaged/1850_photos.json'\n",
    "assert type(count_json_iter(url)) == int\n",
    "assert len(json.loads(requests.get(url).content)) == count_json_iter(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@functools.lru_cache(256)\n",
    "def get_year_size(year: Union[int,str], kind: str) -> dict:\n",
    "    \"\"\"returns size of a json dataset for a given year and kind\n",
    "    results are cached\n",
    "    Parameters\n",
    "    ----------\n",
    "    year : Union[int,str]\n",
    "        year from newspaper navigator\n",
    "    kind : str\n",
    "        {'ads', 'photos', 'maps', 'illustrations', 'comics', 'cartoons', 'headlines'}\n",
    "    Returns\n",
    "    -------\n",
    "    size :dict\n",
    "        returns a dict with year as a key and size as value\n",
    "    \"\"\"\n",
    "    session = None\n",
    "    dset_size = {}\n",
    "    url = get_json_url(year,kind)\n",
    "    if kind == 'ads' and int(year) >=1870 or (kind == 'headlines'):\n",
    "        session = create_session()\n",
    "    dset_size[str(year)] = count_json_iter(url, session)\n",
    "    return dset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1850': 22}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_year_size(1850, 'photos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@functools.lru_cache(512)\n",
    "def get_year_sizes(kind,start=1850, end=1950, step=5):\n",
    "    \"\"\"\n",
    "    Returns the sizes for json data files for `kind` between year `start` and `end`\n",
    "    with step size 'step'\n",
    "\n",
    "    Parameters:\n",
    "    kind (str): kind of image from news-navigator:\n",
    "    {'ads', 'photos', 'maps', 'illustrations', 'comics', 'cartoons', 'headlines'}\n",
    "\n",
    "    Returns:\n",
    "    Pandas.DataFrame: holding data from input json url\n",
    "    \"\"\"\n",
    "  #  dset_size = {}\n",
    "    futures = []\n",
    "    years = range(start,end+1,step)\n",
    "    max_workers = get_max_workers(years)\n",
    "    with tqdm(total=len(years)) as progress:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            for year in years:\n",
    "                future = executor.submit(get_year_size, year, kind)\n",
    "                future.add_done_callback(lambda p: progress.update())\n",
    "                futures.append(future)\n",
    "        results = [future.result() for future in futures]\n",
    "        dset_size = {k: v for d in results for k, v in d.items()}\n",
    "    return pd.DataFrame.from_dict(\n",
    "        dset_size, orient='index', columns=[f'{kind}_count']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the year sizes for a given kind taking a step size `step`. For example to get the number of photos in the news-navigator dataset between 1850 and 1860 for every year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940e917b8817425f9e23cb0cac50db1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 52.6 ms, sys: 33.1 ms, total: 85.7 ms\n",
      "Wall time: 94.2 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>photos_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1850</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1854</th>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1855</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      photos_count\n",
       "1850            22\n",
       "1851            20\n",
       "1852            22\n",
       "1853            45\n",
       "1854           221\n",
       "1855            17"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_year_sizes('photos',1850, 1855, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd0cf24e5544c708ea0d238bb23fb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a29566e6f5497389fa2a01938faf39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "assert len(get_year_sizes('photos',1850, 1860, step=1)) == 11\n",
    "assert len(get_year_sizes('photos',1850,1860, step=2)) == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_all_year_sizes(start=1850, end=1950,step=1, save:bool=True):\n",
    "    \"\"\"\n",
    "    Returns a dataframe with number of counts from year `start` to `end`\n",
    "    \"\"\"\n",
    "    kinds = ['ads', 'photos', 'maps', 'illustrations',\n",
    "                    'comics', 'cartoons', 'headlines']\n",
    "    dfs = []\n",
    "    for kind in tqdm(kinds):\n",
    "        df = get_year_sizes(kind, start=start, end=end, step=step)\n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs, axis=1)\n",
    "    df['total'] = df.sum(axis=1)\n",
    "    if save:\n",
    "        df.to_csv('all_year_sizes.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming sampling\n",
    "\n",
    "Since we want a subset of the Newspaper Navigator datasets which we can either work with for [annotation](!TODO add link) or for inference we want to create samples. Sampling in python can be complicated depending on the type of population you are working with and the properties your sample needs to have but usually we can do something fairly simple like. For example, if we want to sample from a selection of books we could do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['War and Peace']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "books = ['War and Peace', 'Frankenstein', 'If They Come in the Morning']\n",
    "random.sample(books, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we run into a same problem as when trying to get the length of a json dataset which wouldn't fit into memory above. For example if we want to sample $k$ examples from one of our json files which we can't load into memory. To get around this we can use [Reservoir_sampling](https://en.wikipedia.org/wiki/Reservoir_sampling):\n",
    "\n",
    "> Reservoir sampling is a family of randomized algorithms for choosing a simple random sample without replacement of k items from a population of unknown size n in a single pass over the items. The size of the population n is not known to the algorithm and is typically too large to fit all n items into main memory. The population is revealed to the algorithm over time, and the algorithm cannot look back at previous items. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sample_stream(stream, k:int):\n",
    "    \"\"\"\n",
    "    Return a random sample of k elements drawn without replacement from stream.\n",
    "    Designed to be used when the elements of stream cannot easily fit into memory.\n",
    "    \"\"\"\n",
    "    r = np.array(list(itertools.islice(stream, k)))\n",
    "    for t, x in enumerate(stream, k + 1):\n",
    "        i = np.random.randint(1, t + 1)\n",
    "        if i <= k:\n",
    "            r[i - 1] = x\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we sample whilst only loading a small numer of items into memory at one time. This does come at some cost, mainly speed. There are faster ways of sampling from a stream but this isn't the main bottle neck for sampling in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([37650, 92023, 31564, 28726, 81691])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_stream(range(1,100000), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Karl Marx', 'Rosa Luxenburg'], dtype='<U14')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['Karl Marx', 'Rosa Luxenburg', 'Raya Dunayevskaya', 'CLR James']\n",
    "sample_stream(iter(names), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert len(sample_stream(range(1,100),5)) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@functools.lru_cache(1024)\n",
    "def calc_frac_size(url,frac, session=None):\n",
    "    \"returns fraction size from a json stream\"\n",
    "    return round(count_json_iter(url,session)*frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "url = get_json_url(1850)\n",
    "assert calc_frac_size(url, 0.5)== 11 #22*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def calc_year_from_total(total,start,end,step):\n",
    "    \"Calculate size of a year sample based on a total sample size\"\n",
    "    return max(1, round(total/(((end-start)+1)/step)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_year_from_total(10,1850, 18950,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "assert calc_year_from_total(10, 1850,1950,1) >=1 # test that a value is always returned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def reduce_df_memory(df):\n",
    "    return df.astype(\n",
    "            {\"score\": \"float64\",\n",
    "                \"page_seq_num\": \"int32\",\n",
    "                \"batch\": \"category\",\n",
    "                \"box\":\"object\",\n",
    "                \"lccn\": \"category\",\n",
    "                \"page_url\": \"category\",\n",
    "                \"name\": \"category\",\n",
    "                \"publisher\": \"category\",\n",
    "                \"place_of_publication\": \"category\",\n",
    "                \"edition_seq_num\": \"category\"}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class nnSampler:\n",
    "    \"\"\"\n",
    "    Sampler for creating samples from Newspaper Navigator data\n",
    "    \"\"\"\n",
    "    population = pd.read_csv(pkg_resources.resource_stream('nnanno', 'data/all_year_counts.csv'), index_col=0)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f'{self.__class__.__name__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sample_year(kind:str, sample_size:Union[int,float], year:int) ->np.array:\n",
    "    url = get_json_url(year, kind)\n",
    "    if kind == 'ads' and int(year) >=1870 or (kind == 'headlines'):\n",
    "        session = create_session()\n",
    "    else:\n",
    "        session = create_cached_session()\n",
    "    if type(sample_size) is float:\n",
    "        sample_size = max(1,calc_frac_size(url, sample_size, session))\n",
    "        if kind == 'ads' and int(year) >=1870 or (kind == 'headlines'):\n",
    "            session = create_session()\n",
    "        else:\n",
    "            session = create_cached_session()\n",
    "    with session.get(get_json_url(year, kind)) as r:\n",
    "        if r:\n",
    "            try:\n",
    "                data = ijson.items(r.content, \"item\")\n",
    "                sample_data = sample_stream(iter(data), sample_size)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                sample_data = np.nan\n",
    "        return sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_year('photos', 1, 1850)\n",
    "assert len(sample_year('maps', 0.1, 1850)) == 1 # test we always have a sample size of at least one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = sample_year('ads',0.1,1920)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch_to(nnSampler)\n",
    "def create_sample(self,\n",
    "                  sample_size: Union[int, float],\n",
    "                  kind: str = \"photos\",\n",
    "                  start_year: int = 1850,\n",
    "                  end_year: int = 1950,\n",
    "                  step: int = 5,\n",
    "                  year_sample=True,\n",
    "                  save: bool = False,\n",
    "                  reduce_memory=True):\n",
    "                \"\"\"\n",
    "                Creates a sample of Newspaper Navigator data for a given set of years and a kind\n",
    "\n",
    "                Parameters:\n",
    "                sample_size: int, float\n",
    "                    `sample size` can either be a fixed number or a fraction of the total dataset size\n",
    "                kind (str): kind of image from news-navigator:\n",
    "                {'ads', 'photos', 'maps', 'illustrations', 'comics', 'cartoons', 'headlines'}\n",
    "\n",
    "\n",
    "                Returns:\n",
    "                Pandas.DataFrame: holding data from input json url\n",
    "                \"\"\"\n",
    "\n",
    "                if not year_sample:\n",
    "                    if type(sample_size) != int:\n",
    "                        raise ValueError(\n",
    "                            f\"\"\"type{sample_size} is not an int. Fractions are only supported\n",
    "                            for sampling by year\"\"\"\n",
    "                        )\n",
    "                    sample_size = calc_year_from_total(sample_size, start_year, end_year, step)\n",
    "                futures = []\n",
    "                years = range(start_year, end_year + 1, step)\n",
    "                _year_sample = partial(sample_year, kind, sample_size)\n",
    "                with tqdm(total=len(years)) as progress:\n",
    "                    workers = get_max_workers(years)\n",
    "                    with concurrent.futures.ThreadPoolExecutor(1) as executor:\n",
    "                        for year in years:\n",
    "                            future = executor.submit(_year_sample, year)\n",
    "                            future.add_done_callback(lambda p: progress.update())\n",
    "                            futures.append(future)\n",
    "                results = [future.result() for future in futures]\n",
    "                df = pd.DataFrame.from_dict(list(itertoolz.concat(results)))\n",
    "\n",
    "                if reduce_memory:\n",
    "                    df = reduce_df_memory(df)\n",
    "                if save:\n",
    "                    df.to_json(f\"{kind}_{start_year}_{end_year}_sample.json\")\n",
    "                self.sample = df\n",
    "                return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch_to(nnSampler)\n",
    "def download_sample(self,\n",
    "            out_dir: str,\n",
    "            json_name: Optional[str]=None,\n",
    "            df: Optional[pd.DataFrame] = None,\n",
    "            original: bool = True,\n",
    "            pct: Optional[int] = None,\n",
    "            size: Optional[tuple] = None,\n",
    "            preserve_asp_ratio: bool = True) -> Union[None]:\n",
    "            \"\"\"Download images associated with a sample\n",
    "            The majority of paramters relate to the options available in a IIIF image request\n",
    "            see `https://iiif.io/api/image/3.0/#4-image-requests` for further information\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            out_dir\n",
    "                The save directory for the images\n",
    "            json_name\n",
    "\n",
    "            df\n",
    "                optional DataFrame containing a sample\n",
    "            original\n",
    "                if `True` will download orginal size images via IIIF\n",
    "            pct\n",
    "                optional value which scales the size of images requested by `pct`\n",
    "            size\n",
    "                a tuple representing `width` by `height`, will be passed to IIIF request\n",
    "            preserve_asp_ratio\n",
    "                whether to ask the IIIF request to preserve aspect ratio of image or not\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            None\n",
    "                Nothing is returned by a\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            if df is not None:\n",
    "                self.download_df = df.copy(deep=True)\n",
    "            else:\n",
    "                try:\n",
    "                    self.download_df = self.sample.copy(deep=True)\n",
    "                except AttributeError as E:\n",
    "                    print(\n",
    "                        \"You need to create a sample before downloading, or pass in a previously created \"\n",
    "                    )\n",
    "            self.download_df[\"iiif_url\"] = self.download_df.apply(\n",
    "                lambda x: iiif_df_apply(\n",
    "                    x,\n",
    "                     original=original,\n",
    "                     pct=pct,\n",
    "                     size=size,\n",
    "                     preserve_asp_ratio=preserve_asp_ratio,\n",
    "                ),\n",
    "                axis=1,\n",
    "            )\n",
    "            self.download_df[\"download_image_path\"] = self.download_df['filepath'].str.replace('/','_')\n",
    "\n",
    "            if not Path(out_dir).exists():\n",
    "                Path(out_dir).mkdir(parents=True)\n",
    "            _download_image = lambda x: download_image(\n",
    "                x.iiif_url, x.download_image_path, out_dir)\n",
    "            with tqdm(total=len(self.download_df)) as progress:\n",
    "                workers = get_max_workers(self.download_df)\n",
    "                with concurrent.futures.ThreadPoolExecutor(workers) as executor:\n",
    "                    futures = []\n",
    "                    for tuple_row in self.download_df.itertuples():\n",
    "                        future = executor.submit(_download_image, tuple_row)\n",
    "                        future.add_done_callback(lambda p: progress.update())\n",
    "                        futures.append(future)\n",
    "                    del futures\n",
    "            if json_name is None:\n",
    "                today = datetime.today()\n",
    "                time_stamp = today.strftime(\"%Y_%d_%m_%H_%M\")\n",
    "                json_name = f\"{time_stamp}_{len(self.download_df)}_sample\"\n",
    "            self.download_df.to_json(f'{out_dir}/{json_name}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = nnSampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nnSampler"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ads_count</th>\n",
       "      <th>photos_count</th>\n",
       "      <th>maps_count</th>\n",
       "      <th>illustrations_count</th>\n",
       "      <th>comics_count</th>\n",
       "      <th>cartoons_count</th>\n",
       "      <th>headlines_count</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1850-01-01</th>\n",
       "      <td>8841</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>671</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>11243</td>\n",
       "      <td>20791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851-01-01</th>\n",
       "      <td>10065</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>457</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>12262</td>\n",
       "      <td>22817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852-01-01</th>\n",
       "      <td>8764</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>671</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>13524</td>\n",
       "      <td>23009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853-01-01</th>\n",
       "      <td>11517</td>\n",
       "      <td>45</td>\n",
       "      <td>5</td>\n",
       "      <td>1106</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "      <td>13224</td>\n",
       "      <td>25986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1854-01-01</th>\n",
       "      <td>15050</td>\n",
       "      <td>221</td>\n",
       "      <td>15</td>\n",
       "      <td>732</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>15282</td>\n",
       "      <td>31314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1946-01-01</th>\n",
       "      <td>185139</td>\n",
       "      <td>5945</td>\n",
       "      <td>1857</td>\n",
       "      <td>1053</td>\n",
       "      <td>3280</td>\n",
       "      <td>861</td>\n",
       "      <td>68275</td>\n",
       "      <td>266410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1947-01-01</th>\n",
       "      <td>181223</td>\n",
       "      <td>4188</td>\n",
       "      <td>1750</td>\n",
       "      <td>1115</td>\n",
       "      <td>3630</td>\n",
       "      <td>797</td>\n",
       "      <td>57018</td>\n",
       "      <td>249721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948-01-01</th>\n",
       "      <td>152987</td>\n",
       "      <td>4282</td>\n",
       "      <td>1359</td>\n",
       "      <td>1154</td>\n",
       "      <td>3031</td>\n",
       "      <td>624</td>\n",
       "      <td>43432</td>\n",
       "      <td>206869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949-01-01</th>\n",
       "      <td>154510</td>\n",
       "      <td>6015</td>\n",
       "      <td>1888</td>\n",
       "      <td>1280</td>\n",
       "      <td>3356</td>\n",
       "      <td>634</td>\n",
       "      <td>42904</td>\n",
       "      <td>210587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950-01-01</th>\n",
       "      <td>154961</td>\n",
       "      <td>5630</td>\n",
       "      <td>1952</td>\n",
       "      <td>1223</td>\n",
       "      <td>3893</td>\n",
       "      <td>704</td>\n",
       "      <td>37854</td>\n",
       "      <td>206217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ads_count  photos_count  maps_count  illustrations_count  \\\n",
       "1850-01-01       8841            22           5                  671   \n",
       "1851-01-01      10065            20           6                  457   \n",
       "1852-01-01       8764            22          10                  671   \n",
       "1853-01-01      11517            45           5                 1106   \n",
       "1854-01-01      15050           221          15                  732   \n",
       "...               ...           ...         ...                  ...   \n",
       "1946-01-01     185139          5945        1857                 1053   \n",
       "1947-01-01     181223          4188        1750                 1115   \n",
       "1948-01-01     152987          4282        1359                 1154   \n",
       "1949-01-01     154510          6015        1888                 1280   \n",
       "1950-01-01     154961          5630        1952                 1223   \n",
       "\n",
       "            comics_count  cartoons_count  headlines_count   total  \n",
       "1850-01-01             9               0            11243   20791  \n",
       "1851-01-01             7               0            12262   22817  \n",
       "1852-01-01            10               8            13524   23009  \n",
       "1853-01-01            88               1            13224   25986  \n",
       "1854-01-01            11               3            15282   31314  \n",
       "...                  ...             ...              ...     ...  \n",
       "1946-01-01          3280             861            68275  266410  \n",
       "1947-01-01          3630             797            57018  249721  \n",
       "1948-01-01          3031             624            43432  206869  \n",
       "1949-01-01          3356             634            42904  210587  \n",
       "1950-01-01          3893             704            37854  206217  \n",
       "\n",
       "[101 rows x 8 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "decdb404973a4f20841bd3c9162e2bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>page_seq_num</th>\n",
       "      <th>edition_seq_num</th>\n",
       "      <th>batch</th>\n",
       "      <th>lccn</th>\n",
       "      <th>box</th>\n",
       "      <th>score</th>\n",
       "      <th>ocr</th>\n",
       "      <th>place_of_publication</th>\n",
       "      <th>geographic_coverage</th>\n",
       "      <th>name</th>\n",
       "      <th>publisher</th>\n",
       "      <th>url</th>\n",
       "      <th>page_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ohi_ingstad_ver01/data/sn85026051/00296027029/...</td>\n",
       "      <td>1850-07-27</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>ohi_ingstad_ver01</td>\n",
       "      <td>sn85026051</td>\n",
       "      <td>[0.29913574490319106, 0.622813938380955, 0.430...</td>\n",
       "      <td>0.980025</td>\n",
       "      <td>[ht, I, ', Wll., ., III, tl, T, ., \"', \"', \", ...</td>\n",
       "      <td>Fremont, Sandusky County, Ohio</td>\n",
       "      <td>[Ohio--Sandusky--Fremont]</td>\n",
       "      <td>Fremont weekly freeman. [volume]</td>\n",
       "      <td>J.S. Fouke</td>\n",
       "      <td>https://news-navigator.labs.loc.gov/data/ohi_i...</td>\n",
       "      <td>https://chroniclingamerica.loc.gov/data/batche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ohi_ingstad_ver01/data/sn85026051/00296027029/...</td>\n",
       "      <td>1850-07-20</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>ohi_ingstad_ver01</td>\n",
       "      <td>sn85026051</td>\n",
       "      <td>[0.3009427797781111, 0.6294158908847332, 0.433...</td>\n",
       "      <td>0.929614</td>\n",
       "      <td>[L, -, COME, IN,, WE, CALL, YOU, !, .v';:]</td>\n",
       "      <td>Fremont, Sandusky County, Ohio</td>\n",
       "      <td>[Ohio--Sandusky--Fremont]</td>\n",
       "      <td>Fremont weekly freeman. [volume]</td>\n",
       "      <td>J.S. Fouke</td>\n",
       "      <td>https://news-navigator.labs.loc.gov/data/ohi_i...</td>\n",
       "      <td>https://chroniclingamerica.loc.gov/data/batche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ncu_hawk_ver02/data/sn84026472/00416156360/185...</td>\n",
       "      <td>1850-05-22</td>\n",
       "      <td>289</td>\n",
       "      <td>1</td>\n",
       "      <td>ncu_hawk_ver02</td>\n",
       "      <td>sn84026472</td>\n",
       "      <td>[0.6732673909317263, 0.042179068056539225, 0.8...</td>\n",
       "      <td>0.914908</td>\n",
       "      <td>[]</td>\n",
       "      <td>Hillsborough, N.C.</td>\n",
       "      <td>[North Carolina--Orange--Hillsboro]</td>\n",
       "      <td>The Hillsborough recorder. [volume]</td>\n",
       "      <td>Dennis Heartt</td>\n",
       "      <td>https://news-navigator.labs.loc.gov/data/ncu_h...</td>\n",
       "      <td>https://chroniclingamerica.loc.gov/data/batche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ohi_ingstad_ver01/data/sn85026051/00296027029/...</td>\n",
       "      <td>1850-12-07</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>ohi_ingstad_ver01</td>\n",
       "      <td>sn85026051</td>\n",
       "      <td>[0.30707743987524494, 0.6473851770787806, 0.44...</td>\n",
       "      <td>0.984739</td>\n",
       "      <td>[COME, IN,, WE, CALL, YOU!]</td>\n",
       "      <td>Fremont, Sandusky County, Ohio</td>\n",
       "      <td>[Ohio--Sandusky--Fremont]</td>\n",
       "      <td>Fremont weekly freeman. [volume]</td>\n",
       "      <td>J.S. Fouke</td>\n",
       "      <td>https://news-navigator.labs.loc.gov/data/ohi_i...</td>\n",
       "      <td>https://chroniclingamerica.loc.gov/data/batche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ohi_ingstad_ver01/data/sn85026051/00296027029/...</td>\n",
       "      <td>1850-08-17</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>ohi_ingstad_ver01</td>\n",
       "      <td>sn85026051</td>\n",
       "      <td>[0.2943367379610656, 0.6305186744386874, 0.426...</td>\n",
       "      <td>0.956234</td>\n",
       "      <td>[COME, IN,, WE, CALL, YOU, !, o]</td>\n",
       "      <td>Fremont, Sandusky County, Ohio</td>\n",
       "      <td>[Ohio--Sandusky--Fremont]</td>\n",
       "      <td>Fremont weekly freeman. [volume]</td>\n",
       "      <td>J.S. Fouke</td>\n",
       "      <td>https://news-navigator.labs.loc.gov/data/ohi_i...</td>\n",
       "      <td>https://chroniclingamerica.loc.gov/data/batche...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filepath    pub_date  \\\n",
       "0  ohi_ingstad_ver01/data/sn85026051/00296027029/...  1850-07-27   \n",
       "1  ohi_ingstad_ver01/data/sn85026051/00296027029/...  1850-07-20   \n",
       "2  ncu_hawk_ver02/data/sn84026472/00416156360/185...  1850-05-22   \n",
       "3  ohi_ingstad_ver01/data/sn85026051/00296027029/...  1850-12-07   \n",
       "4  ohi_ingstad_ver01/data/sn85026051/00296027029/...  1850-08-17   \n",
       "\n",
       "   page_seq_num edition_seq_num              batch        lccn  \\\n",
       "0            37               1  ohi_ingstad_ver01  sn85026051   \n",
       "1            33               1  ohi_ingstad_ver01  sn85026051   \n",
       "2           289               1     ncu_hawk_ver02  sn84026472   \n",
       "3           115               1  ohi_ingstad_ver01  sn85026051   \n",
       "4            49               1  ohi_ingstad_ver01  sn85026051   \n",
       "\n",
       "                                                 box     score  \\\n",
       "0  [0.29913574490319106, 0.622813938380955, 0.430...  0.980025   \n",
       "1  [0.3009427797781111, 0.6294158908847332, 0.433...  0.929614   \n",
       "2  [0.6732673909317263, 0.042179068056539225, 0.8...  0.914908   \n",
       "3  [0.30707743987524494, 0.6473851770787806, 0.44...  0.984739   \n",
       "4  [0.2943367379610656, 0.6305186744386874, 0.426...  0.956234   \n",
       "\n",
       "                                                 ocr  \\\n",
       "0  [ht, I, ', Wll., ., III, tl, T, ., \"', \"', \", ...   \n",
       "1         [L, -, COME, IN,, WE, CALL, YOU, !, .v';:]   \n",
       "2                                                 []   \n",
       "3                        [COME, IN,, WE, CALL, YOU!]   \n",
       "4                   [COME, IN,, WE, CALL, YOU, !, o]   \n",
       "\n",
       "             place_of_publication                  geographic_coverage  \\\n",
       "0  Fremont, Sandusky County, Ohio            [Ohio--Sandusky--Fremont]   \n",
       "1  Fremont, Sandusky County, Ohio            [Ohio--Sandusky--Fremont]   \n",
       "2              Hillsborough, N.C.  [North Carolina--Orange--Hillsboro]   \n",
       "3  Fremont, Sandusky County, Ohio            [Ohio--Sandusky--Fremont]   \n",
       "4  Fremont, Sandusky County, Ohio            [Ohio--Sandusky--Fremont]   \n",
       "\n",
       "                                  name      publisher  \\\n",
       "0     Fremont weekly freeman. [volume]     J.S. Fouke   \n",
       "1     Fremont weekly freeman. [volume]     J.S. Fouke   \n",
       "2  The Hillsborough recorder. [volume]  Dennis Heartt   \n",
       "3     Fremont weekly freeman. [volume]     J.S. Fouke   \n",
       "4     Fremont weekly freeman. [volume]     J.S. Fouke   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://news-navigator.labs.loc.gov/data/ohi_i...   \n",
       "1  https://news-navigator.labs.loc.gov/data/ohi_i...   \n",
       "2  https://news-navigator.labs.loc.gov/data/ncu_h...   \n",
       "3  https://news-navigator.labs.loc.gov/data/ohi_i...   \n",
       "4  https://news-navigator.labs.loc.gov/data/ohi_i...   \n",
       "\n",
       "                                            page_url  \n",
       "0  https://chroniclingamerica.loc.gov/data/batche...  \n",
       "1  https://chroniclingamerica.loc.gov/data/batche...  \n",
       "2  https://chroniclingamerica.loc.gov/data/batche...  \n",
       "3  https://chroniclingamerica.loc.gov/data/batche...  \n",
       "4  https://chroniclingamerica.loc.gov/data/batche...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sampler.create_sample(sample_size=10, kind='photos', start_year=1850,end_year=1855,reduce_memory=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading a sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186ca6f8a3f041149cefdfccd3a3fb04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83dc6b80f9ef40b8933c2a142787b3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampler.create_sample(sample_size=10, kind='ads', start_year=1850,end_year=1850,reduce_memory=True)\n",
    "sampler.download_sample('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "files = [f for f in Path('test/').iterdir()]; json_file = list(Path('test').glob('*.json')); df = pd.read_json(json_file[0])\n",
    "assert len(df) == 10\n",
    "# check iiif urls from df return at least some images \n",
    "iiif_url_load_results = map(load_url_image, df['iiif_url'])\n",
    "assert any(type(result) == PIL.Image.Image for result in iiif_url_load_results) == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#tidyup\n",
    "files = [f for f in Path('test/').iterdir()]; list(map(Path.unlink, files))\n",
    "Path('test').rmdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_sample.ipynb.\n",
      "Converted 02_annotate.ipynb.\n",
      "Converted 03_inference.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnAnnotator",
   "language": "python",
   "name": "nnannotator"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

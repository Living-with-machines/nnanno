{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create inference dataframe for testing inference code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import ijson\n",
    "import pkg_resources\n",
    "import pandas as pd\n",
    "from cytoolz import itertoolz\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnanno.core import *\n",
    "from nnanno.sample import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import (\n",
    "    Any,\n",
    "    Optional,\n",
    "    Union,\n",
    "    Dict,\n",
    "    List,\n",
    "    Tuple,\n",
    "    Set,\n",
    "    Iterable,\n",
    ")\n",
    "from PIL import Image\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = ImageDataLoaders.from_csv('../ph/ads/', 'ads_upsampled.csv',folder='images', fn_col='file', label_col='label',item_tfms=Resize(64,ResizeMethod.Squish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(dls, resnet18, metrics=F1Score())\n",
    "learn.fine_tune(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nnPredict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing images\n",
    "Because we are dealing with images requested via the web we have to deal with the occasional hiccup. This hiccup could include requested image not being returned from an IIIF request, or a network issue etc. The method we use to load images is defined in `core`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?load_url_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_url_image` will sometimes return `None`. When we're running inference this can cause an issue because we want to create batches of images to speed up inference. We don't want to include `None`s in a batch of images to predict. To get around this we create a function which filters a batch of images and replaces `None` with a fake image. This function also returns the index of items which were originally `None`. This allows us to use this index of items which were None to replace any predictions made for dummy images with `np.nan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _filter_replace_none_image(results:List[Optional[PIL.Image.Image]]):\n",
    "    fakeim = Image.fromarray(244 * np.ones((250,250,3), np.uint8))\n",
    "    results = L(results)\n",
    "    none_image_index = results.argwhere(lambda x: x is None) # Gets the index for images which are none\n",
    "    results[none_image_index] = fakeim # Replaces None with fakeim\n",
    "    return results.items, none_image_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# TODO replace get_image_files with test images\n",
    "im_files = (get_image_files('../ph/ads/images'))[:8] \n",
    "results = list(map(PILImage.create,im_files))\n",
    "results.append(None)\n",
    "results = [None] + results\n",
    "image_batch,none_image_index =_filter_replace_none_image(results)\n",
    "assert len(results) == len(image_batch)\n",
    "assert none_image_index.items == [0,9] # check indexes are at the start and end of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://news-navigator.labs.loc.gov/data/dlc_fiji_ver01/data/sn83030214/00175040936/1900102801/0519/001_0_99.jpg'\n",
    "im = load_url_image(url);im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_files = (get_image_files('../ph/ads/images'))[:4]\n",
    "images = list(map(PILImage.create, im_files))\n",
    "results = [None] + images \n",
    "images,index =_filter_replace_none_image(results)\n",
    "\n",
    "image_batch = [np.array(im) for im in images]\n",
    "test_data = learn.dls.test_dl(image_batch)\n",
    "pred_tuple = learn.get_preds(dl=test_data, with_decoded=True)\n",
    "pred_decoded = L(pred_tuple[2], use_list=True)\n",
    "pred_tensor =  L(pred_tuple[0],use_list=True)\n",
    "pred_decoded[index] = np.nan; pred_tensor[index] = np.nan\n",
    "pred_decoded.items, pred_tensor.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://news-navigator.labs.loc.gov/data/dlc_fiji_ver01/data/sn83030214/00175040936/1900102801/0519/001_0_99.jpg'\n",
    "im = load_url_image(url)\n",
    "images = [im,im,im]\n",
    "results = [None] + images \n",
    "images,index =_filter_replace_none_image(results)\n",
    "image_batch = [np.array(im) for im in images]\n",
    "list(map(np.shape,image_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _create_pred_header(fname, dls=None):\n",
    "    columns=[\n",
    "            \"filepath\",\n",
    "            \"pub_date\",\n",
    "            \"page_seq_num\",\n",
    "            \"edition_seq_num\",\n",
    "            \"batch\",\n",
    "            \"lccn\",\n",
    "            \"box\",\n",
    "            \"score\",\n",
    "            \"ocr\",\n",
    "            \"place_of_publication\",\n",
    "            \"geographic_coverage\",\n",
    "            \"name\",\n",
    "            \"publisher\",\n",
    "            \"url\",\n",
    "            \"page_url\",\n",
    "            \"iiif_url\",\n",
    "            \"pred_decoded\"]\n",
    "    if dls:\n",
    "        columns = columns + (list(dls.vocab))\n",
    "    return pd.DataFrame(columns=columns).to_csv(fname, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "_create_pred_header('test_header.csv')\n",
    "df = pd.read_csv('test_header.csv')\n",
    "assert df.columns.to_list() == ['filepath', 'pub_date', 'page_seq_num', 'edition_seq_num', 'batch',\n",
    "       'lccn', 'box', 'score', 'ocr', 'place_of_publication',\n",
    "       'geographic_coverage', 'name', 'publisher', 'url', 'page_url',\n",
    "       'iiif_url', 'pred_decoded']\n",
    "Path('test_header.csv').unlink()\n",
    "_create_pred_header('test_header.csv', dls=dls)\n",
    "df = pd.read_csv('test_header.csv')\n",
    "assert len(df[dls.vocab].columns) == dls.c\n",
    "Path('test_header.csv').unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_year_csv(out_dir, year,kind,dls=None):\n",
    "    fname = Path(f\"{out_dir}/{year}_{kind}.csv\")\n",
    "    _create_pred_header(fname, dls)\n",
    "    return fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_year_json(out_dir, year,kind, batch):\n",
    "    fname = Path(f\"{out_dir}/{year}_{kind}_{batch}.json\")\n",
    "    return fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "Path('test_csv').mkdir()\n",
    "_create_year_csv('test_csv',1850,'ads')\n",
    "assert Path('test_csv/1850_ads.csv').exists() == True\n",
    "Path('test_csv/1850_ads.csv').unlink() ;Path('test_csv/').rmdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO how to save to json \n",
    "\n",
    "# TODO save to csv a bit more nicely "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class nnPredict:\n",
    "    def __init__(self, learner, try_gpu=True):\n",
    "        self.learner = learner\n",
    "        self.try_gpu = try_gpu\n",
    "        self.population = pd.read_csv(pkg_resources.resource_stream('nnanno', 'data/all_year_counts.csv'), \n",
    "                                      index_col=0)\n",
    "    def _get_year_sample_size(self, kind,year):\n",
    "        return self.population[f\"{kind}_count\"][year]\n",
    "    \n",
    "    def predict_from_sample_df(self, sample_df,bs=16):\n",
    "        # TODO docstring\n",
    "        self.sample_df = sample_df\n",
    "       # Path(out_dir).mkdir(exist_ok=True)\n",
    "        if self.try_gpu:\n",
    "            if torch.cuda.is_available:\n",
    "                gpu = True\n",
    "            else:\n",
    "                gpu = False\n",
    "        if gpu:\n",
    "            self.learner.model = self.learner.model.cuda() \n",
    "        self.sample_df['iiif_url'] = self.sample_df.apply(lambda x: iiif_df_apply(x,size=(250,250)),axis=1)\n",
    "        dfs = []\n",
    "        splits = round(len(self.sample_df)/bs)\n",
    "        for df in tqdm(np.array_split(sample_df, splits)):\n",
    "            futures=[]\n",
    "            for url in df['iiif_url'].to_list():\n",
    "                with ThreadPoolExecutor() as e:\n",
    "                    future = e.submit(load_url_image,url)\n",
    "                    futures.append(future)\n",
    "            results = [future.result() for future in futures]\n",
    "            image_list, none_index = _filter_replace_none_image(results)\n",
    "            im_as_arrays = [np.array(image) for image in image_list]\n",
    "            if len(none_index) >0:\n",
    "                        tqdm.write(f\"{none_index} skipped\")\n",
    "            else:\n",
    "                pass\n",
    "            test_data = self.learner.dls.test_dl(im_as_arrays)\n",
    "            if gpu:\n",
    "                test_data.to('cuda') \n",
    "            with self.learner.no_bar():\n",
    "                pred_tuple = self.learner.get_preds(dl=test_data, with_decoded=True)\n",
    "            pred_decoded = L(pred_tuple[2], use_list=True)\n",
    "            pred_tensor =  L(pred_tuple[0],use_list=None)\n",
    "            pred_decoded[none_index] = np.nan; pred_tensor[none_index] = np.nan\n",
    "            df[\"pred_decoded\"] = pred_decoded.items\n",
    "            df[\"pred_decoded\"] = df['pred_decoded'].astype(float)\n",
    "            # create an empty df column for each class in dls.vocab\n",
    "            for c in dls.vocab:\n",
    "                df[f'{c}_prob'] = ''\n",
    "            # append the tensor predictions to the last `c` colomns of the df\n",
    "            df.iloc[:,-dls.c:] = np.hsplit(pred_tensor.numpy(),dls.c) #split into columns\n",
    "            #df.to_csv('test.csv', header=None, index=None, mode=\"a\")\n",
    "            dfs.append(df)\n",
    "        return dfs\n",
    "\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        kind: str,\n",
    "        out_dir: str,\n",
    "        bs: int = 32,\n",
    "        sample_size: Union[int, float] = None,\n",
    "        start_year: int = 1850,\n",
    "        end_year: int = 1950,\n",
    "        step: int = 1,\n",
    "        year_sample:bool=True,\n",
    "    ):\n",
    "        if Path(out_dir).exists() and len(os.scandir(out_dir)) >=1:\n",
    "            raise ValueError(f'{out_fn} already exists and is not empty')\n",
    "        Path(out_dir).mkdir(exist_ok=True)\n",
    "#         if sample_size and not year_sample:\n",
    "#             if not type(sample_size) == int:\n",
    "#                 raise ValueError(\n",
    "#                     f\"type{sample_size} is not an int. Fractions are only supported for sampling by year\"\n",
    "#                 )\n",
    "#             sample_size = calc_year_from_total(sample_size, start_year, end_year, step)\n",
    "        if self.try_gpu:\n",
    "            if torch.cuda.is_available():\n",
    "                gpu = True\n",
    "                print('using gpu')\n",
    "            else:\n",
    "                gpu = False\n",
    "        if gpu:\n",
    "            self.learner.model = self.learner.model.cuda() \n",
    "        years = range(start_year, end_year + 1, step)\n",
    "        total = self._get_year_sample_size(kind,years).sum()\n",
    "        pbar = tqdm(years,total=total)\n",
    "        for year in pbar:\n",
    "            out_fn = _create_year_csv(out_dir,year,kind, dls)\n",
    "            pbar.set_description(f\"Predicting: {year}, total progress\")\n",
    "            if kind == ('ads' and int(year) >=1870) or (kind == 'headlines'):\n",
    "                s = create_session()\n",
    "            else:\n",
    "                s = create_cached_session() \n",
    "            with s.get(get_json_url(year, kind), timeout=60) as r: \n",
    "                if r.from_cache:\n",
    "                    tqdm.write('using cache')\n",
    "                data = ijson.items(r.content, \"item\")\n",
    "                # TODO add sample approach\n",
    "                batches = itertoolz.partition_all(bs, iter(data))\n",
    "                year_total = self._get_year_sample_size(kind,year)\n",
    "                for i,batch in enumerate(tqdm(\n",
    "                    batches, total=round(year_total//bs),leave=False, desc='Batch Progress')):\n",
    "                    df = pd.DataFrame(batch)\n",
    "                    df[\"iiif_url\"] = df.apply(lambda x: iiif_df_apply(x), axis=1)\n",
    "                    futures = []\n",
    "                    workers = get_max_workers(df)\n",
    "                    for iif_url in df[\"iiif_url\"].values:\n",
    "                        with concurrent.futures.ThreadPoolExecutor(workers) as e:\n",
    "                            future = e.submit(load_url_image, iif_url)\n",
    "                            futures.append(future)\n",
    "                    results = [future.result() for future in futures]\n",
    "                    image_list, none_index = _filter_replace_none_image(results)\n",
    "                    im_as_arrays = [np.array(image) for image in image_list]\n",
    "                    if len(none_index) >0:\n",
    "                        tqdm.write(f\"{none_index} skipped\")\n",
    "                    else:\n",
    "                        pass\n",
    "                    test_data = learn.dls.test_dl(im_as_arrays)\n",
    "                    with self.learner.no_bar():\n",
    "                        pred_tuple = self.learner.get_preds(dl=test_data, with_decoded=True)\n",
    "                    pred_decoded = L(pred_tuple[2], use_list=True)\n",
    "                    pred_tensor =  L(pred_tuple[0],use_list=None)\n",
    "                    pred_decoded[none_index] = np.nan; pred_tensor[none_index] = np.nan\n",
    "                    df[\"pred_decoded\"] = pred_decoded.items\n",
    "                    df[\"pred_decoded\"] = df['pred_decoded'].astype(float)\n",
    "                    # create an empty df column for each class in dls.vocab\n",
    "                    for c in dls.vocab:\n",
    "                        df[f'{c}_prob'] = ''\n",
    "                    # append the tensor predictions to the last `c` colomns of the df\n",
    "                    df.iloc[:,-dls.c:] = np.hsplit(pred_tensor.numpy(),dls.c) #split into columns\n",
    "                    df.to_csv(out_fn, header=None, index=None, mode=\"a\")\n",
    "                    pbar.update(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_sample.ipynb.\n",
      "Converted 02_annotate.ipynb.\n",
      "Converted 03_inference.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nnanno]",
   "language": "python",
   "name": "conda-env-nnanno-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
